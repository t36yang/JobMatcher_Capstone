{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HYNb1BR4IhPK",
    "outputId": "5d021def-b834-47d6-baac-28d3b716e04f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Shen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/Shen/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Shen/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Shen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# preprocessing\n",
    "import re\n",
    "import string\n",
    "import itertools # for flattening\n",
    "# nltk library\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem.wordnet import WordNetLemmatizer as wn\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer # lemmatizer using WordNet\n",
    "from nltk.corpus import wordnet # imports WordNet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/Shen/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=0, _provided_cursor=None, _parent=None, _block_type=None, _form_data=None)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.title(\"Job Matcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=0, _provided_cursor=None, _parent=None, _block_type=None, _form_data=None)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.image('streamlit.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=0, _provided_cursor=None, _parent=None, _block_type=None, _form_data=None)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.markdown(\"Enter all your skills that you can think of and separate with a comma! This system will return the jobs that align with your skillset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=1, _provided_cursor=None, _parent=DeltaGenerator(_root_container=0, _provided_cursor=None, _parent=None, _block_type=None, _form_data=None), _block_type=None, _form_data=None)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add Sidebar\n",
    "st.sidebar.markdown(\"## Match the jobs that align with your skillset\")\n",
    "st.sidebar.caption(\"About This System:\")\n",
    "st.sidebar.caption(\"This recommendation system is constructed using cosine similarities between your skillset and more than 22000 job descriptions that were trained utilizing NLTK NMF Natural Language Processing algorithm.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=1, _provided_cursor=None, _parent=DeltaGenerator(_root_container=0, _provided_cursor=None, _parent=None, _block_type=None, _form_data=None), _block_type=None, _form_data=None)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sidebar cont.\n",
    "st.sidebar.markdown(\"#### Informationa about the database:\")\n",
    "st.sidebar.caption(\"Downloaded from Kaggle: Dice Tech Job Board\")\n",
    "st.sidebar.caption(\"Due to data constrains, please note that this system is only matching the jobs in the tech industry.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=1, _provided_cursor=None, _parent=DeltaGenerator(_root_container=0, _provided_cursor=None, _parent=None, _block_type=None, _form_data=None), _block_type=None, _form_data=None)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sidebar cont.\n",
    "st.sidebar.markdown(\"#### Author: Vickie Yang\")\n",
    "st.sidebar.caption(\"Github: https://github.com/t36yang\")\n",
    "st.sidebar.caption(\"LinkedIn: www.linkedin.com/in/yangvickie\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill=st.text_input(\"Please enter your skills\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0OusKz7o3jkt",
    "outputId": "ad999b43-b0b4-4855-82ba-d2be660059d8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "userinput= skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=pd.read_pickle('cleanedfile_dicejob.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df[['City','State','Other']]=new_df.joblocation_address.str.split(\", \", expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=new_df.drop('Other', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.State=new_df.State.str.upper().apply(lambda x : \"TX\" if x == \"TEXAS\" else \"DC\" if x == \"WASHINGTON\" else \"NY\" if x == \"SPRINGS\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.write(\"---\")\n",
    "selectedState = st.selectbox('Select State:',\n",
    "                    new_df.State.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('You selected: ', 1, 'jobs')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.write(\"---\")\n",
    "'###### No. of Recommended Jobs to display'\n",
    "jobs=st.slider('# of Recommended Jobs',0,100,1)\n",
    "'You selected: ', jobs, 'jobs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_process(text):\n",
    "    \n",
    "    # get common stop words that we'll remove during tokenization/text normalization\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    #initialize lemmatizer\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    # helper function to change nltk's part of speech tagging to a wordnet format.\n",
    "    def pos_tagger(nltk_tag):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:         \n",
    "            return None\n",
    "   \n",
    "\n",
    "    # lower case everything\n",
    "    txt_lower = text.lower()\n",
    "\n",
    "    #remove mentions, hashtags, and urls, strip whitspace and breaks\n",
    "    txt_lower = re.sub(r\"@[a-z0-9_]+|#[a-z0-9_]+|http\\S+\", \"\", txt_lower).strip().replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "    \n",
    "    #remove words with short length\n",
    "    \n",
    "    # remove stop words and punctuations \n",
    "    txt_norm = [x for x in word_tokenize(txt_lower) if ((x.isalpha()) & (x not in stop_words)) & (x not in ['good','great','found','company','lot','experience','fit','candidate','applicant','requirement','qualification','Deloitte','professional','year','application','opportunity','description','work','role','need','email','delivery',\"req_id\",\"job_req\",\"req\",\"id\",\"please\",\"resume\",\"position\",\"forward\",\"receive\",\"contact\",\"minimum\",\"required\",\"disability\",\"eligibility\",\"employment\",\"team\",\"click\"])]\n",
    "\n",
    "    #  POS detection on the result will be important in telling Wordnet's lemmatizer how to lemmatize\n",
    "    \n",
    "    # creates list of tuples with tokens and POS tags in wordnet format\n",
    "    txt_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tag(txt_norm))) \n",
    "\n",
    "    # lemmatize the input\n",
    "    txt_processed = \" \".join([wnl.lemmatize(x[0], x[1]) for x in txt_tagged if x[1] is not None])\n",
    "    return txt_processed\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "if st.button('Show Jobs'):\n",
    "    \n",
    "    with open('bestmodel.pkl' , 'rb') as f:\n",
    "        lr = pickle.load(f)\n",
    "        \n",
    "    userresume=input_process(userinput)\n",
    "\n",
    "    vectorized_col = new_df['joined_bigram']\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train = vectorizer.fit_transform(vectorized_col)\n",
    "\n",
    "\n",
    "    resume_vec = vectorizer.transform([userresume])\n",
    "\n",
    "    resume_transform=lr.transform(resume_vec)\n",
    "    sim = cosine_similarity(X_train, resume_vec)\n",
    "\n",
    "    dic = {}\n",
    "    for i,x in enumerate(sim):\n",
    "      dic[i] = x\n",
    "\n",
    "    highest_sim = pd.DataFrame(dic).T.sort_values(by = 0, ascending = False).head(jobs).index\n",
    "    \n",
    "    st.write(\"Jobs:\")\n",
    "    recommend=new_df.iloc[highest_sim, :]\n",
    "    st.dataframe(recommend.loc[recommend['State']==selectedState][['jobtitle','jobdescription',\"company\",'advertiserurl']].rename(columns={\"Job Title\": 'jobtitle',\"Job Description\" : 'jobdescription',\"Company\": 'company','Link':'advertiserurl'}))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "bjNrt7V0f1zy",
    "7cqdvcZtIhP6",
    "TJ_w0HM4IhP6",
    "ByYg-C8IahGg",
    "UhtGZG2UIhP8",
    "KUiFNAXbIhP8",
    "3xs03v8WIhP-",
    "OXQRteDuZEeG"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
